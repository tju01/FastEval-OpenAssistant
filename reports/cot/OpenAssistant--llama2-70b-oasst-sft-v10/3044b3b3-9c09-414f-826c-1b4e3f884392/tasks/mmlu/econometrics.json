{
    "score": 0.2,
    "model_outputs": [
        {
            "id": 0,
            "question": "Which one of the following is the most appropriate definition of a 99% confidence interval?\n\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter\n(B) 99% of the time in repeated samples, the interval would contain the estimated value of the parameter\n(C) 99% of the time in repeated samples, the null hypothesis will be rejected\n(D) 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false",
            "correct_answer": 0,
            "model_answer": "To answer this question, let's go through each option and evaluate their appropriateness as a definition of a 99% confidence interval:\n\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter:\nThis statement describes the idea that we are 99% confident that the true value of the parameter lies within the interval. This definition is commonly used for confidence intervals and accurately reflects their meaning.\n\n(B) 99% of the time in repeated samples, the interval would contain the estimated value of the parameter:\nThis statement is incorrect because the estimated value of the parameter is a single value, not a range. Confidence intervals are intervals that contain an estimated value with a certain level of confidence, so this definition does not accurately describe a confidence interval.\n\n(C) 99% of the time in repeated samples, the null hypothesis will be rejected:\nThis statement is related to hypothesis testing and not confidence intervals. In hypothesis testing, we reject or fail to reject the null hypothesis based on the evidence provided by the data. Confidence intervals, on the other hand, are used to express the uncertainty surrounding an estimated value.\n\n(D) 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false:\nThis statement is also related to hypothesis testing and is not an appropriate definition of a confidence interval. The statement describes a scenario where the null hypothesis is not rejected even though it is false, but this does not define a confidence interval.\n\nBased on the evaluation of each option, the most appropriate definition of a 99% confidence interval is:\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter.\nThis definition accurately describes the idea that we are 99% confident that the true value of the parameter lies within the interval.",
            "correct": false
        },
        {
            "id": 1,
            "question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?\n\n(A) ADF is a single equation approach to unit root testing while PP is a systems approach\n(B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test\n(C) The PP test incorporates an automatic correction for autocorrelated residuals in the test regression\n(D) PP tests have good power in small samples whereas DF tests do not.",
            "correct_answer": 2,
            "model_answer": "To answer this question, we need to understand the key differences between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing.\n\nThe DF test is a popular test for a unit root in a time series. It is a single-equation approach that tests whether a series has a unit root or is stationary around a deterministic trend. The null hypothesis of the DF test is that the series has a unit root, while the alternative hypothesis is that the series is stationary.\n\nThe PP test, on the other hand, is an extension of the DF test that allows for the possibility of serially correlated errors in the regression equation. It also uses a single-equation approach but with a transformed variable to test for stationarity. The null and alternative hypotheses of the PP test are reversed compared to the DF test, so that the null hypothesis is that the series is stationary and the alternative hypothesis is that it has a unit root.\n\nNow let's examine each of the answer choices and compare them to our understanding of the DF and PP tests:\n\n(A) This statement is true. The DF test is a single-equation approach, while the PP test is a systems approach. However, this is not the main difference between the two tests.\n\n(B) This statement is also true. The null and alternative hypotheses of the PP test are indeed reversed compared to the DF test. This is one of the key differences between the two tests.\n\n(C) This statement is false. Neither the DF test nor the PP test incorporates an automatic correction for autocorrelated residuals in the test regression.\n\n(D) This statement is also false. Both the DF test and the PP test have good power in large samples but may have poor power in small samples.\n\nBased on our analysis, the main difference between the DF and PP approaches to unit root testing is that the null and alternative hypotheses of the PP test are reversed compared to the DF test. Therefore, the correct answer is:\n\n(B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test.",
            "correct": false
        },
        {
            "id": 2,
            "question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?\n\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude\n(B) It would be symmetrical about zero\n(C) It would rise less quickly for negative disturbances than for positive ones of the same magnitude\n(D) It would be zero for all positive disturbances",
            "correct_answer": 0,
            "model_answer": "The presence of a leverage effect in a model would imply that the impact of news on asset prices is magnified when the asset is highly leveraged. In other words, when an asset has a high degree of leverage, even small changes in the underlying fundamentals can lead to large changes in the asset's price.\n\nGiven this understanding of the leverage effect, we can now analyze the shape of the news impact curve for a model that accounts for this effect.\n\nFor negative disturbances, the leverage effect would cause the asset price to fall more quickly than it would without leverage. This is because the negative news would lead to a larger change in the asset's fundamentals relative to the asset's value, and the leveraged position would amplify this effect.\n\nSimilarly, for positive disturbances, the leverage effect would cause the asset price to rise more quickly than it would without leverage. This is because the positive news would lead to a larger change in the asset's fundamentals relative to the asset's value, and the leveraged position would again amplify this effect.\n\nTherefore, the shape of the news impact curve for a model that accounted for a leverage effect would be such that it rises more quickly for negative disturbances than for positive ones of the same magnitude.\n\nSo the answer is:\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude",
            "correct": true
        },
        {
            "id": 3,
            "question": "Which of the following statements is false concerning the linear probability model?\n\n(A) There is nothing in the model to ensure that the estimated probabilities lie between zero and one\n(B) Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one\n(C) The error terms will be heteroscedastic and not normally distributed\n(D) The model is much harder to estimate than a standard regression model with a continuous dependent variable",
            "correct_answer": 3,
            "model_answer": "Let's evaluate each statement one by one:\n\n(A) The linear probability model assumes a linear relationship between a continuous dependent variable and its explanatory variables. The dependent variable is transformed into a probability using a standard normal cumulative distribution function. This transformation ensures that the estimated probabilities will always lie between zero and one. Therefore, this statement is false.\n\n(B) If the probabilities are truncated at zero and one, it means that some observations will have a probability of zero or one. However, the linear probability model assumes a continuous relationship between the dependent and independent variables. As a result, the probabilities should not be exactly zero or one for any observation. Thus, this statement is also false.\n\n(C) The linear probability model assumes that the error terms are normally distributed with a constant variance. However, in reality, the variances of the error terms may not be constant, especially when the probabilities are close to zero or one. This is known as heteroscedasticity. Therefore, this statement is true.\n\n(D) The linear probability model is easier to estimate than a standard regression model with a continuous dependent variable because it does not require the assumption of a specific functional form for the relationship between the dependent and independent variables. The linear probability model only assumes a linear relationship, which is a weaker assumption. Hence, this statement is false.\n\nAfter evaluating all the statements, we can conclude that the true statement is:\n\n(C) The error terms will be heteroscedastic and not normally distributed.",
            "correct": false
        },
        {
            "id": 4,
            "question": "Which of the following statements concerning the regression population and sample is FALSE?\n\n(A) The population is the total collection of all items of interest\n(B) The population can be infinite\n(C) In theory, the sample could be larger than the population\n(D) A random sample is one where each individual item from the population is equally likely to be drawn.",
            "correct_answer": 2,
            "model_answer": "Let's examine each statement step-by-step and determine if it is false:\n\n(A) The population is the total collection of all items of interest: This statement is true. The population refers to the entire group of items or individuals that we are interested in studying.\n\n(B) The population can be infinite: This statement is also true. In some cases, the population may be infinite or very large, such as in the case of all people in the world or all possible customers for a product.\n\n(C) In theory, the sample could be larger than the population: This statement is false. A sample must always be smaller than or equal to the population it is drawn from. If the sample were larger than the population, it would not be a representative sample.\n\n(D) A random sample is one where each individual item from the population is equally likely to be drawn: This statement is true. A random sample is one where each item in the population has an equal probability of being selected, which helps to ensure that the sample is representative of the population.\n\nBased on this analysis, the correct answer is:\nC",
            "correct": false
        },
        {
            "id": 5,
            "question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?\n\n(A) Asymptotically, the values of the two test statistics will be equal\n(B) The Q test has better small-sample properties than the Q*\n(C) The Q test is sometimes over-sized for small samples\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.",
            "correct_answer": 1,
            "model_answer": "Let's evaluate each statement step-by-step:\n\n(A) Asymptotically, the values of the two test statistics will be equal: False. Although both tests are used to detect serial correlation in time series, they are not equivalent and their values will not be equal even asymptotically.\n\n(B) The Q test has better small-sample properties than the Q* test: False. The Ljung-Box Q* test is known to have better small-sample properties and is less sensitive to violations of the underlying assumptions compared to the Box-Pierce Q test.\n\n(C) The Q test is sometimes over-sized for small samples: True. The Box-Pierce Q test can be over-sized for small samples, meaning it can reject the null hypothesis more often than the nominal significance level.\n\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients: False. While both tests have a tendency to reject the null hypothesis with larger sample sizes, this statement implies that both tests are always rejecting the null, which is not the case.\n\nBased on the evaluation, the INCORRECT statement is:\n(A) Asymptotically, the values of the two test statistics will be equal",
            "correct": false
        },
        {
            "id": 6,
            "question": "A parsimonious model is one that\n\n(A) Includes too many variables\n(B) Includes as few variables as possible to explain the data\n(C) Is a well-specified model\n(D) Is a mis-specified model",
            "correct_answer": 0,
            "model_answer": "To answer this question, let's evaluate each option in relation to the definition of a parsimonious model:\n\nA parsimonious model is one that aims to explain the data using the fewest number of variables or parameters possible.\n\n(A) Includes too many variables -> This statement is opposite to the definition of a parsimonious model, which aims to include as few variables as possible.\n\n(B) Includes as few variables as possible to explain the data -> This statement aligns with the definition of a parsimonious model, which aims to minimize the number of variables used to explain the data.\n\n(C) Is a well-specified model -> While a parsimonious model should be well-specified, this option doesn't directly address the criteria of including as few variables as possible.\n\n(D) Is a mis-specified model -> A parsimonious model should not be mis-specified. It should accurately represent the data, but it also aims to minimize the number of variables used.\n\nBased on the evaluation of each option, the correct answer is:\n\n(B) Includes as few variables as possible to explain the data",
            "correct": false
        },
        {
            "id": 7,
            "question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?\n\n(A) They can be interpreted as continuously compounded changes in the prices\n(B) They can be added over time to give returns for longer time periods\n(C) They can be added across a portfolio of assets to give portfolio returns\n(D) They are usually fat-tailed",
            "correct_answer": 2,
            "model_answer": "To answer this question, let's go through each option and evaluate whether it is a feature of continuously compounded returns (log-returns) or not.\n\n(A) They can be interpreted as continuously compounded changes in the prices.\n\nTrue. Continuously compounded returns represent the continuous growth of an investment over time. They can be interpreted as the continuously compounded changes in the prices of an asset.\n\n(B) They can be added over time to give returns for longer time periods.\n\nTrue. One of the key features of log-returns is that they can be cumulated over time to give the total return for a longer time period. This is known as the geometric series formula.\n\n(C) They can be added across a portfolio of assets to give portfolio returns.\n\nTrue. Log-returns can be used to calculate the returns for individual assets in a portfolio, and these returns can then be combined to give the overall portfolio return. The weighted average of the log-returns of the individual assets gives the log-return of the portfolio.\n\n(D) They are usually fat-tailed.\n\nFalse. Continuously compounded returns (log-returns) follow a lognormal distribution, which has thinner tails than a normal distribution. Fat-tailed distributions have a higher probability of observing large price movements, which is not a characteristic of log-returns.\n\nTherefore, the answer is (D) They are usually fat-tailed, as this is NOT a feature of continuously compounded returns (log-returns).\n\n(D)",
            "correct": false
        },
        {
            "id": 8,
            "question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns\n\n(A) (ii) and (iv) only\n(B) (i) and (iii) only\n(C) (i), (ii), and (iii) only\n(D) (i), (ii), (iii), and (iv)",
            "correct_answer": 1,
            "model_answer": "First, let's discuss what a GARCH(1,1) model is and what it captures.\n\nA GARCH(1,1) model is a small variation of a GARCH model where we use the previous squared error term (or residual) in addition to previous variance term. Specifically, it is defined as:\n\n\\begin{equation}\nr_t = \\mu + \\sigma_t \\epsilon_t, \\ \\ \\sigma^2_t = \\alpha_0 + \\alpha_1 r_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2,\n\\end{equation}\n\nwhere $r_t$ is the return at time $t$, $\\sigma_t$ is the conditional volatility at time $t$, $\\mu$ is the mean return, $\\epsilon_t$ is the error term, $\\sigma^2_t$ is the variance of $\\epsilon_t$ (which we are modeling), $\\alpha_0$ and $\\alpha_1$ are parameters controlling the impact of the squared past error term, and $\\beta_1$ is a parameter controlling the impact of the past variance term.\n\nNow let's discuss what each of the features mean:\n\ni) Fat tails in the return distribution: This means returns are asymmetric, the probability of extreme events (either very good or very bad returns) is much higher than what a normal distribution would predict.\n\nii) Leverage effects: This refers to the positive correlation between past returns and future volatility. Essentially when we have negative shocks (low returns) we expect future volatility to increase.\n\niii) Volatility clustering: This refers to the phenomena that large shocks/events tend to cluster together. Essentially high volatility tends to be followed by high volatility and vice versa.\n\niv) Volatility affecting returns: This refers to the idea that when volatility is high we expect lower returns (and vice versa). It can also refer to the fact that there is a positive correlation between past volatility and future returns.\n\nFrom defining a GARCH(1,1) we know that it can capture feature ii) (leverage effects) thanks to the past squared error term we're using but we cannot capture volatility affecting returns ( feature iv). \nTo capture iii) (volatility clustering), we'd need to include the past variance term in the equation for mean return to capture the feedback between returns and volatility in the GARCH equation. This requires a more complex version of the model called a GJR-GARCH model.\nTo capture fat tails (feature i), you would need to tweak the distribution that you assume for your error terms in the model. A student-t distribution would give you thicker tails for example.\n\nSo the correct answer is: (A) (ii) and (iv) only",
            "correct": false
        },
        {
            "id": 9,
            "question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?\n\n(A) Less than -1\n(B) Between -1 and 0\n(C) Between 0 and 1\n(D) Bigger than 1",
            "correct_answer": 2,
            "model_answer": "To answer this question, let's first briefly recap the GARCH-M model:\n\nThe GARCH-M model is an extension of the GARCH model that includes a mean equation in addition to the variance equation. The GARCH-in-mean parameter estimate represents the degree to which past squared errors affect the current mean return.\n\nNow, let's consider the possible values of the GARCH-in-mean parameter estimate:\n\n(A) Less than -1: This would imply that past squared errors have a very strong negative effect on the current mean return, which is unlikely.\n\n(B) Between -1 and 0: This would imply that past squared errors have a moderate negative effect on the current mean return, which is possible but not typical.\n\n(C) Between 0 and 1: This would imply that past squared errors have a positive effect on the current mean return, which is more likely since it is common for stock returns to exhibit momentum.\n\n(D) Bigger than 1: This would imply that past squared errors have a very strong positive effect on the current mean return, which is unlikely.\n\nBased on the above reasoning, the most likely value for the GARCH-in-mean parameter estimate would be:\n\n(C) Between 0 and 1",
            "correct": true
        }
    ]
}